# -*- coding: utf-8 -*-
"""onlineRetailML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GT_bcP4vJzO0mjkxQ96AUyGfZoNkArKh
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

df = pd.read_csv('/content/drive/MyDrive/Datasets/Level 2 Online Retail/onlineRetailFeatureEngineered.csv')
customerRfmDf = pd.read_csv('/content/drive/MyDrive/Datasets/Level 2 Online Retail/customersRfm.csv')
monthlyRevenueDf = pd.read_csv('/content/drive/MyDrive/Datasets/Level 2 Online Retail/monthlyRevenue.csv')

# grouping by ID, Year and Month
monthlyCustomerDf = df.groupby(['Customer ID', 'Year', 'Month'])['TotalPrice'].sum().reset_index()
monthlyCustomerDf

# merging with rfm
mergedDf = monthlyCustomerDf.merge(customerRfmDf, on='Customer ID', how='left')
mergedDf

# creating Next Month Price target
# for each customer we take the next row’s value in TotalPrice and assign it to the current row
mergedDf = mergedDf.sort_values(['Customer ID', 'Year', 'Month'])
mergedDf['Total Price Next Month'] = mergedDf.groupby('Customer ID')['TotalPrice'].shift(-1)
mergedDf = mergedDf.dropna(subset=['Total Price Next Month'])
mergedDf

# merging with monthly revenue df
mergedDfFinal = mergedDf.merge(monthlyRevenueDf, on=['Year', 'Month'], how='left')
mergedDfFinal

# preparing data
from sklearn.model_selection import train_test_split, GridSearchCV

X = mergedDfFinal.drop(['Customer ID', 'Total Price Next Month'], axis=1)
y = mergedDfFinal['Total Price Next Month']

xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.2, random_state=42)

# preprocessing and pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

numFeatures = ['Year','Month','TotalPrice','Recency','Frequency','Monetary','Total Revenue']
preprocessor = ColumnTransformer(
    transformers=[('num', StandardScaler(), numFeatures)]
)

# model selection and metrics
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score

models = {
    'Linear Regression': LinearRegression(n_jobs=-1),
    'Random Forest Regression': RandomForestRegressor(n_estimators=100, criterion='squared_error',
                                                      random_state=42),
    'XGBoost': GradientBoostingRegressor(n_estimators=100, random_state=42)
}

for name, model in models.items():
  model.fit(xTrain, yTrain)
  preds = model.predict(xTest)
  print(name)
  print(f"MAE: {mean_absolute_error(yTest, preds)}")
  print(f"MAEP: {mean_absolute_percentage_error(yTest, preds)}")
  print(f"R2 Score: {r2_score(yTest, preds)}")
  print("------")

# looking at error distribution
for name, model in models.items():
  modelPreds = model.predict(xTest)
  residuals = yTest - modelPreds
  plt.scatter(yTest, modelPreds, label=f"{name}", ec="black",
              color="#9127F5")
  plt.legend(loc="best")
  plt.show()

# training and hyperparameter tuning
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(n_estimators=100, criterion='squared_error',
                                                      random_state=42)),
])

paramGrid = {
    'regressor__n_estimators': [100, 200],
    'regressor__criterion': ['squared_error', 'absolute_error'],
    'regressor__max_depth': [None, 2, 5],
    'regressor__min_samples_split': [2, 5]
}

gridSearch = GridSearchCV(pipeline, paramGrid, cv=3)
gridSearch.fit(xTrain, yTrain)

# metrics
bestModel = gridSearch.best_estimator_
bestModelPreds = bestModel.predict(xTest)
print(f"Best parameters: {gridSearch.best_params_}")
print(f"Best score: {gridSearch.best_score_}")
print(f"Metrics: ")
print(f"MAE: {mean_absolute_error(yTest, bestModelPreds)}")
print(f"MAPE: {mean_absolute_percentage_error(yTest, bestModelPreds)}")
print(f"R2 Score: {r2_score(yTest, bestModelPreds)}")

import joblib
joblib.dump(bestModel, '/content/drive/MyDrive/Datasets/Level 2 Online Retail/bestModel.pkl')

"""### Conclusion and Findings

I trained three regression models — Linear Regression, Random Forest, and XGBoost — to predict next month’s total price for each customer.

Linear Regression achieved the highest R² score, indicating that it best captured the overall linear trend and explained the largest portion of variance in the target variable.
However, it also showed higher MAE and MAPE, suggesting that while it fit the global structure well, it struggled with local deviations and outliers.

Random Forest and XGBoost produced lower R² scores (~0.47), but smaller absolute errors. This is expected since these ensemble models are better at handling non-linear relationships and noise, leading to more stable and accurate point predictions even when they don’t explain variance as completely as the linear model.
"""