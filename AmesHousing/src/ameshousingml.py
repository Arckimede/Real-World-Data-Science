# -*- coding: utf-8 -*-
"""amesHousingML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P9_fLSSpnnADN7w_f_TenOgrSZSTc85I
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/Datasets/Ames Housing Level 2/amesHousingFeatureEngineering.csv', keep_default_na=False, na_values=[])
df.head()

# dropping Order and PID since we don't need them
df.drop(columns=['Order', 'PID'], inplace=True)

# variables most correlated with SalePrice for regression
corrs = df.corr(method='pearson', numeric_only=True)
salesPriceCorr = corrs['SalePrice'].sort_values(ascending=False).head(15)
salesPriceCorr

# setting up the features needed and target
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer

X = df[['Overall Qual Gr Liv', 'Overall Qual', 'Gr Liv Area', 'Exter Qual',
        'Kitchen Qual', 'Total Bsmt SF', 'Garage Cars', 'Garage Area',
        'Total Rooms Gr Liv', '1st Flr SF', 'Total Bathrooms', 'Bsmt Qual',
        'Year Built', 'Built Decade']]
y = df['SalePrice']

numCols = ['Overall Qual Gr Liv', 'Overall Qual', 'Gr Liv Area', 'Exter Qual',
        'Kitchen Qual', 'Total Bsmt SF', 'Garage Cars', 'Garage Area',
        'Total Rooms Gr Liv', '1st Flr SF', 'Total Bathrooms', 'Bsmt Qual',
        'Year Built', 'Built Decade']
preprocessor = ColumnTransformer(
    transformers=[('num', StandardScaler(), numCols)], n_jobs=-1)

# splitting into train and test sets
xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.2, random_state=42)

# training models
from sklearn.linear_model import RidgeCV

# best alpha for Ridge regressor
alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]
ridgeCV = RidgeCV(alphas=alphas, store_cv_results=True)
ridgeCV.fit(xTrain, yTrain)

print(f'Best alpha: {ridgeCV.alpha_}')

# best Random Forest regressor
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor

randForestRegr = RandomForestRegressor(n_estimators=100)

gridParams = {
    'n_estimators': [100, 200, 500],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

gridSearchCV = GridSearchCV(estimator=randForestRegr, param_grid=gridParams,
                            scoring='neg_mean_squared_error', cv=5, n_jobs=-1)
gridSearchCV.fit(xTrain, yTrain)
print(gridSearchCV.best_params_)

from sklearn.linear_model import LinearRegression, Ridge
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error

# testing the best models
models = {
    'Linear Regr.': LinearRegression(n_jobs=-1),
    'Ridge Regr.': Ridge(alpha=10),
    'Random Forest': RandomForestRegressor(n_estimators=200, max_depth=None,
                min_samples_leaf=2, min_samples_split=5, n_jobs=-1)
}

results = {}
bestModel = None
bestScore = -np.inf

# creating the pipeline
for name, model in models.items():
    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])
    pipeline.fit(xTrain, yTrain)
    preds = pipeline.predict(xTest)
    mae = mean_absolute_error(yTest, preds)
    r2Score = r2_score(yTest, preds)
    mape = mean_absolute_percentage_error(yTest, preds)
    results[name] = {'MAE: ': mae, 'R² score: ': r2Score, 'MAPE: ': mape}
    print(f'{name}: MAE: {mae} | R²: {r2Score} | MAPE: {mape}')

    # plotting residuals
    residuals = yTest - preds
    plt.figure(figsize=(8, 5))
    plt.scatter(preds, residuals, edgecolor='black', color='#2780F5', alpha=0.6)
    plt.xlabel('Predicted values')
    plt.ylabel('Residuals')
    plt.title(f'Residual Plot: {name}')
    plt.axhline(y=0, color='#F54927', lw=1.2, linestyle='--')
    plt.tight_layout()
    plt.show()

    if r2Score > bestScore:
        bestModel = model

# plotting metrics (MAE, R² score and MAPE)
maes, r2Scores, mapes = [], [], []
mlMetrics = [metric for metric in list(results.keys())]

for keys, values in results.items():
    for metricKey, metricValue in values.items():
        if metricKey == 'MAE: ':
            maes.append(metricValue)
        elif metricKey == 'R² score: ':
            r2Scores.append(metricValue)
        else:
            mapes.append(metricValue)


fig, axes = plt.subplots(nrows=1, ncols=3, constrained_layout=True,
                         figsize=(12, 6))
axes[0].bar(mlMetrics, maes, color='#F5D627', edgecolor='black', label='MAE')
axes[0].legend(loc='best')
axes[0].grid(alpha=0.2)

axes[1].bar(mlMetrics, r2Scores, color='#27E0F5', edgecolor='black', label='R² score')
axes[1].legend(loc='best')
axes[1].grid(alpha=0.2)

axes[2].bar(mlMetrics, mapes, color='#F52791', edgecolor='black', label='MAPE')
axes[2].legend(loc='best')
axes[2].grid(alpha=0.2)

# best model
print(bestModel)

# saving the best model
import joblib
joblib.dump(bestModel, filename='/content/drive/MyDrive/Datasets/Ames Housing Level 2/bestModel.pkl')

"""# Conclusions

This analysis utilized 14 numerical features to predict the target variable, SalePrice, using three distinct regression models. Hyperparameter tuning was applied to both the Ridge and Random Forest estimators to achieve optimal performance (e.g., Ridge used α=10).

The models were evaluated based on the coefficient of determination (R² score,
MAE, MAPE).

Overall, all three models demonstrated excellent predictive power, successfully explaining 89% to 91% of the variance in the house sale price. The best performing model was the Random Forest Regressor, achieving a MAPE of 9%, meaning the average prediction error is approximately 9% of the actual sale price.

### Residual Analysis and Model Diagnostics
Residual plots are critical for assessing the model's assumptions and predictive consistency.

The residuals for all models are randomly scattered above and below the zero line, without forming any discernible systematic curve. This is the ideal outcome, confirming two important points:

-Unbiased Predictions: The models are not systematically over- or under-predicting the SalePrice.

-Effective Feature Use: The models have successfully captured the underlying patterns in the data (linear and non-linear), as indicated by the high R²
scores.
"""